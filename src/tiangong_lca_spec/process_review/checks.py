"""Core validation and review checks for process datasets."""

from __future__ import annotations

import math
import re
from dataclasses import dataclass
from difflib import SequenceMatcher
from typing import Any, Iterable, Mapping, Sequence

from tiangong_lca_spec.core.exceptions import SpecCodingError

from .models import FieldDefinition, ReviewFinding, ReviewSeverity, SourceRecord

DEFAULT_BALANCE_TOLERANCE = 1e-3
DEFAULT_RELATIVE_TOLERANCE = 1e-2


@dataclass(slots=True)
class ExchangeRecord:
    """Normalized view over an exchange entry."""

    identifier: str | None
    name: str
    direction: str
    amount: float
    unit: str | None
    raw: Mapping[str, Any]


def structural_validation(dataset: Mapping[str, Any]) -> list[ReviewFinding]:
    """Perform lightweight structural validation before review."""
    process = _resolve_process_dataset(dataset)
    findings: list[ReviewFinding] = []
    required_sections = ("processInformation", "modellingAndValidation", "administrativeInformation")
    for section in required_sections:
        if section not in process:
            findings.append(
                ReviewFinding(
                    category="validation",
                    severity="error",
                    message=f"Missing required section `{section}`.",
                    path=section,
                    suggestion="Ensure the process dataset is generated by Stage 3 before review.",
                )
            )
    exchanges = list(_extract_exchanges(process))
    if not exchanges:
        findings.append(
            ReviewFinding(
                category="validation",
                severity="error",
                message="Process dataset does not contain any exchanges.",
                path="exchanges",
                suggestion="Stage 2 and Stage 3 outputs must include at least one exchange entry.",
            )
        )
    return findings


def check_exchange_balance(
    dataset: Mapping[str, Any],
    *,
    tolerance: float = DEFAULT_BALANCE_TOLERANCE,
) -> list[ReviewFinding]:
    """Verify that inputs and outputs balance for each unit bucket."""
    exchanges = [_ for _ in map(_normalise_exchange, _extract_exchanges(_resolve_process_dataset(dataset))) if _]
    if not exchanges:
        return [
            ReviewFinding(
                category="numeric_balance",
                severity="warning",
                message="No exchanges available for numeric balance check.",
                path="exchanges",
            )
        ]

    balances: dict[str, float] = {}
    for exchange in exchanges:
        key = exchange.unit or "__unknown__"
        if exchange.direction.lower() == "input":
            balances[key] = balances.get(key, 0.0) - exchange.amount
        else:
            balances[key] = balances.get(key, 0.0) + exchange.amount

    findings: list[ReviewFinding] = []
    for unit_key, balance in balances.items():
        if math.isnan(balance) or math.isinf(balance):
            findings.append(
                ReviewFinding(
                    category="numeric_balance",
                    severity="error",
                    message=f"Balance calculation failed for unit `{unit_key}`.",
                    suggestion="Inspect exchange amounts for non-numeric values.",
                )
            )
            continue
        if abs(balance) <= tolerance:
            continue
        severity: ReviewSeverity = "warning" if unit_key == "__unknown__" else "error"
        findings.append(
            ReviewFinding(
                category="numeric_balance",
                severity=severity,
                message=f"Inputs and outputs differ by {balance:.6g} for unit `{unit_key}`.",
                suggestion="Revisit exchange amounts or document the inventory gap.",
            )
        )
    return findings


def cross_check_sources(
    dataset: Mapping[str, Any],
    sources: Sequence[SourceRecord],
    *,
    tolerance: float = DEFAULT_RELATIVE_TOLERANCE,
) -> list[ReviewFinding]:
    """Compare exchange amounts against extracted source records."""
    exchanges = [_ for _ in map(_normalise_exchange, _extract_exchanges(_resolve_process_dataset(dataset))) if _]
    if not exchanges:
        return []

    if not sources:
        return [
            ReviewFinding(
                category="source_consistency",
                severity="info",
                message="No structured source records supplied; skipping source consistency check.",
                suggestion="Provide parsed source JSON to enable automated comparisons.",
            )
        ]

    findings: list[ReviewFinding] = []
    for exchange in exchanges:
        match, score = _match_source_record(exchange, sources)
        if not match or score < 0.5:
            findings.append(
                ReviewFinding(
                    category="source_consistency",
                    severity="warning",
                    message=f"No matching source record found for exchange `{exchange.name}`.",
                    path=exchange.identifier,
                    suggestion="Confirm the source JSON contains a paragraph or table entry for this exchange.",
                )
            )
            continue
        if match.quantity is None:
            findings.append(
                ReviewFinding(
                    category="source_consistency",
                    severity="info",
                    message=f"Source `{match.identifier}` lacks a numeric quantity for `{exchange.name}`.",
                    evidence=match.text,
                )
            )
            continue
        delta = abs(exchange.amount - match.quantity)
        reference = abs(match.quantity) if match.quantity else delta
        relative = delta / reference if reference else 0.0
        if delta <= DEFAULT_BALANCE_TOLERANCE or relative <= tolerance:
            continue
        findings.append(
            ReviewFinding(
                category="source_consistency",
                severity="error",
                message=(
                    f"Exchange `{exchange.name}` amount {exchange.amount:.6g} differs from "
                    f"source `{match.identifier}` value {match.quantity:.6g}."
                ),
                path=exchange.identifier,
                evidence=match.text,
                suggestion="Verify transcription accuracy or note justified deviations in the review report.",
            )
        )
    return findings


def check_field_content(
    dataset: Mapping[str, Any],
    *,
    definitions: Sequence[FieldDefinition],
) -> list[ReviewFinding]:
    """Validate textual fields against schema descriptions."""
    process = _resolve_process_dataset(dataset)
    findings: list[ReviewFinding] = []
    for definition in definitions:
        value_present, value = _get_by_path(process, definition.path)
        path_label = ".".join(definition.path)
        description = definition.description
        if not value_present:
            if definition.required:
                findings.append(
                    ReviewFinding(
                        category="field_content",
                        severity="error",
                        message=f"Required field `{path_label}` is missing.",
                        evidence=description,
                    )
                )
            continue
        issues = _evaluate_field_value(value, definition)
        for issue in issues:
            findings.append(
                ReviewFinding(
                    category="field_content",
                    severity=issue,
                    message=f"Field `{path_label}` does not satisfy the configured schema constraints.",
                    evidence=description,
                    suggestion="Review the schema description and update the field value.",
                )
            )
    return findings


def _resolve_process_dataset(dataset: Mapping[str, Any]) -> Mapping[str, Any]:
    if not isinstance(dataset, Mapping):
        raise SpecCodingError("Process dataset must be a mapping")
    if "processDataSet" in dataset and isinstance(dataset["processDataSet"], Mapping):
        return dataset["processDataSet"]
    return dataset


def _extract_exchanges(dataset: Mapping[str, Any]) -> Iterable[Mapping[str, Any]]:
    exchanges = dataset.get("exchanges")
    if isinstance(exchanges, Mapping):
        entries = exchanges.get("exchange")
        if isinstance(entries, list):
            yield from entries
        elif isinstance(entries, Mapping):
            yield entries
        return
    if isinstance(exchanges, list):
        yield from exchanges


def _normalise_exchange(exchange: Mapping[str, Any]) -> ExchangeRecord | None:
    if not isinstance(exchange, Mapping):
        return None
    amount = _coerce_float(exchange.get("meanAmount") or exchange.get("resultingAmount"))
    if amount is None:
        return None
    direction = (exchange.get("exchangeDirection") or "").strip() or "Input"
    identifier = exchange.get("@dataSetInternalID") or exchange.get("dataSetInternalID")
    unit = (
        exchange.get("unit")
        or exchange.get("meanAmountUnit")
        or exchange.get("resultingAmountUnit")
        or _extract_unit_from_flow(exchange)
    )
    name = _build_exchange_label(exchange)
    return ExchangeRecord(
        identifier=str(identifier) if identifier is not None else None,
        name=name,
        direction=direction,
        amount=amount,
        unit=unit,
        raw=exchange,
    )


def _extract_unit_from_flow(exchange: Mapping[str, Any]) -> str | None:
    reference = exchange.get("referenceToFlowDataSet")
    if not isinstance(reference, Mapping):
        return None
    properties = reference.get("flowProperties") or reference.get("common:flowProperties")
    if isinstance(properties, Mapping):
        unit = properties.get("flowProperty")
        if isinstance(unit, Mapping):
            return unit.get("shortDescription") or unit.get("#text")
    labels = reference.get("common:shortDescription")
    if isinstance(labels, list):
        for label in labels:
            if isinstance(label, Mapping):
                text = label.get("#text")
                if isinstance(text, str) and "/" in text:
                    segments = [segment.strip() for segment in text.split(";") if segment.strip()]
                    if segments:
                        return segments[-1]
    return None


def _build_exchange_label(exchange: Mapping[str, Any]) -> str:
    labels: list[str] = []
    reference = exchange.get("referenceToFlowDataSet")
    if isinstance(reference, Mapping):
        short_desc = reference.get("common:shortDescription")
        if isinstance(short_desc, list):
            for entry in short_desc:
                if isinstance(entry, Mapping):
                    text = entry.get("#text")
                    if isinstance(text, str) and text.strip():
                        labels.append(text.strip())
                        break
    name = exchange.get("name")
    if isinstance(name, str) and name.strip():
        labels.append(name.strip())
    if not labels:
        identifier = exchange.get("@dataSetInternalID") or exchange.get("dataSetInternalID")
        if identifier is not None:
            labels.append(str(identifier))
    return labels[0] if labels else "exchange"


def _coerce_float(value: Any) -> float | None:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return None
        try:
            return float(text.replace(",", ""))
        except ValueError:
            return None
    return None


def _match_source_record(
    exchange: ExchangeRecord,
    sources: Sequence[SourceRecord],
) -> tuple[SourceRecord | None, float]:
    best: SourceRecord | None = None
    best_score = 0.0
    target = exchange.name.lower()
    for record in sources:
        label_candidates = [record.identifier, record.text]
        label_candidates.extend(
            str(value) for key, value in record.context.items() if isinstance(key, str) and isinstance(value, str)
        )
        candidates = [candidate for candidate in label_candidates if isinstance(candidate, str) and candidate]
        if not candidates:
            continue
        score = max(SequenceMatcher(None, target, candidate.lower()).ratio() for candidate in candidates)
        if score > best_score:
            best_score = score
            best = record
    return best, best_score


def _get_by_path(container: Mapping[str, Any], path: Sequence[str]) -> tuple[bool, Any]:
    current: Any = container
    for segment in path:
        if not isinstance(current, Mapping):
            return False, None
        if segment not in current:
            return False, None
        current = current[segment]
    return True, current


def _evaluate_field_value(value: Any, definition: FieldDefinition) -> list[ReviewSeverity]:
    issues: list[ReviewSeverity] = []
    expected = definition.expected_type
    if expected == "string":
        if not isinstance(value, str) or not value.strip():
            issues.append("error")
    elif expected == "number":
        if _coerce_float(value) is None:
            issues.append("error")
    elif expected == "multilang":
        if not isinstance(value, list) or not value:
            issues.append("error")
        else:
            missing_text = any(not isinstance(item, Mapping) or not item.get("#text") for item in value)
            missing_lang = any(not isinstance(item, Mapping) or "@xml:lang" not in item for item in value)
            if missing_text or missing_lang:
                issues.append("error")
    elif expected == "mapping":
        if not isinstance(value, Mapping) or not value:
            issues.append("error")
    elif expected == "list":
        if not isinstance(value, list) or not value:
            issues.append("error")
    elif expected == "bool":
        if not isinstance(value, bool) and str(value).lower() not in {"true", "false"}:
            issues.append("error")

    if definition.allowed_values is not None:
        allowed = {candidate.lower() for candidate in definition.allowed_values}
        normalised_value = str(value).strip().lower()
        if normalised_value and normalised_value not in allowed:
            issues.append("warning")

    if definition.pattern:
        pattern = re.compile(definition.pattern)
        if not isinstance(value, str) or not pattern.search(value):
            issues.append("warning")

    return issues
